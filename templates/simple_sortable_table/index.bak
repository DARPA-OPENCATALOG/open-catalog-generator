
<html>
<link rel='stylesheet' href='style.css' type='text/css'/>
<script type='text/javascript' src='jquery-latest.js'></script> 
<script type='text/javascript' src='jquery.tablesorter.min.js'></script> 
<script type='text/javascript'>
$(document).ready(function() 
    { 
        $('#myTable').tablesorter({
		// sort on the first column and third column, order asc 
        	sortList: [[0,0],[1,0]] 
    	}); 
    } 
);   
</script>
<h2>DARPA XDATA</h2>
DARPA's XDATA program seeks to develop open source software to address government Big Data at all stages, from analysis to operations, in the areas of scalable analytics, processing, visualizations, and UIs. This new multi-year effort involves over 25 teams from academia, research labs, and small and large businesses, and includes efforts around Hadoop, Python, R, and other technologies.<br>
 <br>
Although the Department of Defense is no stranger to the problem of data analytics at scale, it is encountering many of the same fundamental challenges that businesses face, given the modern explosion of data acquisition, storage, and processing technologies. XDATA is a $25 million/year DARPA program that seeks to develop computational techniques and software tools for analyzing large volumes of data, both semi-structured (e.g., tabular, relational, categorical) and unstructured (e.g., text documents, message traffic). The central challenges we are addressing include developing scalable algorithms for processing imperfect data in distributed data stores and creating effective human-computer interaction tools for facilitating rapidly customizable visual reasoning for diverse missions. Novel aspects of the XDATA program include the embracing of open-source technologies and a focus on minimizing design-to-testing time of new software by using near continual feedback from users.<br>
<br>
<b>Program Manager:</b><br>
Dr. Christopher White<br>
Christopher.White@darpa.mil<br>
<br>
<b>Projects:</b><br>

<table id='myTable' class='tablesorter'> 
<thead> 
<tr> 
    <th>XData Team</th> 
    <th>Software</th> 
    <th>Category</th>
    <th>Web Page</th> 
    <th>Code</th> 
    <th>Dev Stats</th>
    <th>Description</th> 
    <th>License</th> 
</tr> 
</thead> 
<tbody> 
<TR>
<TD>Boeing/Pitt</TD>
<TD>  SMILE-WIDE:  A scalable Bayesian network library</TD>
<TD>Analytics</TD>
<TD>Pending</TD>
<TD>Pending</TD>
<TD></TD>
<TD>SMILE-WIDE is a scalable Bayesian network library. Initially, it is a version of the SMILE library, as in SMILE With Integrated Distributed Execution. The general approach has been to provide an API similar to the existing API SMILE developers use to build "local", single-threaded applications. However, we provide "vectorized" operations that hide a Hadoop-distributed implementation. Apart from invoking a few idioms like generic Hadoop command line argument parsing, these appear to the developer as if they were executed locally.</TD>
<TD></TD>
<TR>
<TD>Carnegie Mellon University</TD>
<TD>Support Distribution Machines </TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/dougalsutherland/py-sdm'>https://github.com/dougalsutherland/py-sdm</a></TD>
<TD>https://github.com/dougalsutherland/py-sdm.git</TD>
<TD><a href='stats/py-sdm/index.html'>stats</a></TD>
<TD>This is a Python implementation of the nonparametric divergence estimators described by Barnabas Poczos, Liang Xiong, Jeff Schneider (2011). Nonparametric divergence estimation with applications to machine learning on distributions. Uncertainty in Artificial Intelligence. http://autonlab.org/autonweb/20287.html and also their use in support vector machines, as described by Dougal J. Sutherland, Liang Xiong, Barnabas Poczos, Jeff Schneider (2012). Kernels on Sample Sets via Nonparametric Divergence Estimates. http://arxiv.org/abs/1202.0302</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>Blaze</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/ContinuumIO/blaze'>https://github.com/ContinuumIO/blaze</a></TD>
<TD>https://github.com/ContinuumIO/blaze.git</TD>
<TD><a href='stats/blaze/index.html'>stats</a></TD>
<TD>Blaze is the next-generation of NumPy. It is designed as a foundational set of abstractions on which to build out-of-core and distributed algorithms over a wide variety of data sources and to extend the structure of NumPy itself. Blaze allows easy composition of low level computation kernels ( C, Fortran, Numba ) to form complex data transformations on large datasets. In Blaze, computations are described in a high-level language (Python) but executed on a low-level runtime (outside of Python), enabling the easy mapping of high-level expertise to data without sacrificing low-level performance. Blaze aims to bring Python and NumPy into the massively-multicore arena, allowing it to able to leverage many CPU and GPU cores across computers, virtual machines and cloud services.</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>Numba</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/numba/numba'>https://github.com/numba/numba</a></TD>
<TD>https://github.com/numba/numba.git</TD>
<TD><a href='stats/numba/index.html'>stats</a></TD>
<TD>Numba is an Open Source NumPy-aware optimizing compiler for Python sponsored by Continuum Analytics, Inc. It uses the remarkable LLVM compiler infrastructure to compile Python syntax to machine code.<br/><br/>It is aware of NumPy arrays as typed memory regions and so can speed-up code using NumPy arrays. Other, less well-typed code will be translated to Python C-API calls effectively removing the "interpreter" but not removing the dynamic indirection.<br/><br/>Numba is also not a tracing jit. It compiles your code before it gets run either using run-time type information or type information you provide in the decorator.<br/><br/>Numba is a mechanism for producing machine code from Python syntax and typed data structures such as those that exist in NumPy.</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>Bokeh</TD>
<TD>Visualization</TD>
<TD><a href='https://github.com/ContinuumIO/Bokeh'>https://github.com/ContinuumIO/Bokeh</a></TD>
<TD>https://github.com/ContinuumIO/Bokeh.git</TD>
<TD><a href='stats/Bokeh/index.html'>stats</a></TD>
<TD>Bokeh (pronounced bo-Kay or bo-Kuh) is a Python interactive visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide elegant, concise construction of novel graphics in the style of Protovis/D3, while delivering high-performance interactivity over large data to thin clients.</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>Abstract Rendering</TD>
<TD>Visualization</TD>
<TD><a href='http://www.github.com/JosephCottam/AbstractRendering'>http://www.github.com/JosephCottam/AbstractRendering</a></TD>
<TD>https://github.com/JosephCottam/AbstractRendering.git</TD>
<TD><a href='stats/AbstractRendering/index.html'>stats</a></TD>
<TD>Information visualization rests on the idea that a meaningful relationship can be drawn between pixels and data. This is most often mediated by geometric entities (such as circles, squares and text) but always involves pixels eventually to display. In most systems, the pixels are tucked away under levels of abstraction in the rendering system. Abstract Rendering takes the opposite approach: expose the pixels and gain powerful pixel-level control. This pixel-level power is a complement many existing visualization techniques. It is an elaboration on rendering, not an analytic or projection step, so it can be used as epilogue to many existing techniques._x000D_
_x000D_
In standard rendering, geometric objects are projected to an image and represented on that image's discrete pixels. The source space is an abstract canvas that contains logically continuous geometric primitives and the target space is an image that contains discrete colors. Abstract Rendering fits between these two states. It introduces a discretization of the data at the pixel-level, but not necessarily all the way to colors. This enables many pixel-level concerns to be efficiently and concisely captured.</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>CDX</TD>
<TD>Visualization</TD>
<TD><a href='https://github.com/ContinuumIO/cdx'>https://github.com/ContinuumIO/cdx</a></TD>
<TD>https://github.com/ContinuumIO/cdx.git</TD>
<TD><a href='stats/cdx/index.html'>stats</a></TD>
<TD>Visualize the structure of large or complexdatasets; produce guides that help users or algorithmsgauge the quality of various kinds of graphs & plots</TD>
<TD>BSD</TD>
<TR>
<TD>Continuum</TD>
<TD>Stencil</TD>
<TD>Visualization</TD>
<TD><a href='https://github.com/JosephCottam/Stencil'>https://github.com/JosephCottam/Stencil</a></TD>
<TD>https://github.com/JosephCottam/Stencil.git</TD>
<TD><a href='stats/Stencil/index.html'>stats</a></TD>
<TD>Stenci is a grammar-based approach to visualization specification at a higher-level.</TD>
<TD>BSD</TD>
<TR>
<TD>Data Tactics Corporation</TD>
<TD>Vowpal Wabbit</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/JohnLangford/vowpal_wabbit'>https://github.com/JohnLangford/vowpal_wabbit</a></TD>
<TD>https://github.com/JohnLangford/vowpal_wabbit.git</TD>
<TD><a href='stats/vowpal_wabbit/index.html'>stats</a></TD>
<TD>The Vowpal Wabbit (VW) project is a fast out-of-core learning system sponsored by Microsoft Research and (previously) Yahoo! Research. Support is available through the mailing list._x000D_
_x000D_
There are two ways to have a fast learning algorithm: (a) start with a slow algorithm and speed it up, or (b) build an intrinsically fast learning algorithm. This project is about approach (b), and it's reached a state where it may be useful to others as a platform for research and experimentation._x000D_
_x000D_
There are several optimization algorithms available with the baseline being sparse gradient descent (GD) on a loss function (several are available), The code should be easily usable. Its only external dependence is on the boost library, which is often installed by default.</TD>
<TD>BSD</TD>
<TR>
<TD>Data Tactics Corporation</TD>
<TD>Go Circuit</TD>
<TD>Infrastructure</TD>
<TD><a href='http://www.gocircuit.org/'>http://www.gocircuit.org/</a></TD>
<TD>https://code.google.com/p/gocircuit/source/checkout</TD>
<TD><a href='stats/gocircuit/index.html'>stats</a></TD>
<TD>Go Circuit reduces the human development and sustenance costs of complex massively-scaled systems nearly to the level of their single-process counterparts. It is a combination of proven ideas from the Erlang ecosystem of distributed embedded devices and Go's ecosystem of Internet application development. Go Circuit extends the reach of Go's linguistic environment to multi-host/multi-process applications.</TD>
<TD>ALv2</TD>
<TR>
<TD>Georgia Tech Research Corporation</TD>
<TD>Fast Algorithms on Imperfect Heterogeneous Distributed Data</TD>
<TD>Analytics</TD>
<TD><a href='http://www.cc.gatech.edu/~hpark/'>http://www.cc.gatech.edu/~hpark/</a></TD>
<TD>Pending</TD>
<TD></TD>
<TD>(1) Large-scale Nonnegative Matrix Factorization<br/><br/>(2) Hawke's Process: predict future events<br/><br/>(3) Topic modeling, Clustering, Dimension reduction, Outlier detection, Recommendation</TD>
<TD>Pending</TD>
<TR>
<TD>IBM Almaden Research Center</TD>
<TD>SKYLARK: Randomized Numerical Linear Algebra and ML</TD>
<TD>Analytics</TD>
<TD>N/A</TD>
<TD>Public Release Planned for 2014</TD>
<TD></TD>
<TD>Matrix algorithms are the foundation for most methods in data analysis, scientic computing, and engineering applications, and Numerical Linear Algebra (NLA) kernels are the key enabling technology for implementing matrix algorithms on dierent hardware platforms. Without highly ecient and scalable NLA kernels, the challenge of big data cannot be met. In this project, provisionally called skylark, we propose to investigate and implement sketching-based NLA kernels for distributed computing platforms. Sketching reduces dimensionality through randomization, and is one of only a few paradigms that have the potential to deliver a true boost in performance in the presence of massive data, for an extensive class of applications.</TD>
<TD>http://www.eclipse.org/legal/epl-v10.html</TD>
<TR>
<TD>Institute for Creative Technologies / USC</TD>
<TD>Immersive Body-Based Interactions</TD>
<TD>Visualization</TD>
<TD><a href='http://ict.usc.edu/'>http://ict.usc.edu/</a></TD>
<TD>http://code.google.com/p/svnmimir/source/checkout</TD>
<TD><a href='stats/immersive_body-based_interactions/index.html'>stats</a></TD>
<TD>* New interface devices and body based interaction techniques to address human-computer interaction challenges posed by Big Data_x000D_
* Wiggle Interaction Technique: user induced motion to speed visual search.<br/><br/>* Immersive Tablet Based Viewers: low cost 3D virtual reality fly-through's of data sets.<br/><br/>* Multi-touch interfaces: browsing/querying multi-attribute and geospatial data, hosted by SOLR.<br/><br/>* Tablet based visualization controller: eye-free rapid interaction with visualizations.</TD>
<TD></TD>
<TR>
<TD>Johns Hopkins University</TD>
<TD>Embedding Methodology and  Statistics for Big Data</TD>
<TD>Analytics</TD>
<TD><a href='http://www.cis.jhu.edu/~parky/XDATA/xfin-new.html'>http://www.cis.jhu.edu/~parky/XDATA/xfin-new.html</a></TD>
<TD>N/A</TD>
<TD></TD>
<TD>* Fast generation of large graphs<br/><br/>* Fast approximate computation of local graph invariants<br/><br/>* Fast parallelizable graph embedding<br/><br/>* API and Web-service for batch processing graphs across formats</TD>
<TD></TD>
<TR>
<TD>Kitware, Inc.</TD>
<TD>VEGA</TD>
<TD>Visualization</TD>
<TD><a href='https://github.com/trifacta/vega'>https://github.com/trifacta/vega</a></TD>
<TD>https://github.com/trifacta/vega.git</TD>
<TD><a href='stats/vega/index.html'>stats</a></TD>
<TD>Vega is a visualization grammar, a declarative format for creating and saving visualization designs. With Vega you can describe data visualizations in a JSON format, and generate interactive views using either HTML5 Canvas or SVG.</TD>
<TD>http://opensource.org/licenses/BSD-3-Clause</TD>
<TR>
<TD>Kitware, Inc.</TD>
<TD>Visualization Design Environment "14 separate apps"</TD>
<TD>Visualization</TD>
<TD>Pending</TD>
<TD>Pending</TD>
<TD></TD>
<TD>Kitware's Visual Design Environment will enable rapid development of insightful big-data visualization applications with minimal programming.</TD>
<TD></TD>
<TR>
<TD>Kitware, Inc.</TD>
<TD>Tangelo</TD>
<TD>Visualization</TD>
<TD><a href='http://tangelo.kitware.com/'>http://tangelo.kitware.com/</a></TD>
<TD>https://github.com/Kitware/tangelo.git</TD>
<TD><a href='stats/tangelo/index.html'>stats</a></TD>
<TD>In a nutshell, Tangelo is a flexible HTML5 web server architecture that cleanly separates your web applications (pure Javascript, HTML, and CSS) and web services (pure Python), bundled with some great tools to get you started.</TD>
<TD>ALv2</TD>
<TR>
<TD>MDA Information Systems, Inc.</TD>
<TD>OODT</TD>
<TD>Infrastructure</TD>
<TD><a href='http://oodt.apache.org/'>http://oodt.apache.org/</a></TD>
<TD>https://svn.apache.org/repos/asf/oodt/</TD>
<TD><a href='stats/oodt/index.html'>stats</a></TD>
<TD>APACHE OODT is metadata for middleware (and vice versa). It enables transparent access to distributed resources, data discovery and query optimization, and distributed processing and virtual archives. But it's not just for science! It's also a software architecture that enables models for information representation, solutions to knowledge capture problems, unification of technology, data, and metadata.</TD>
<TD>ALv2</TD>
<TR>
<TD>MDA Information Systems, Inc.</TD>
<TD>Wings</TD>
<TD>Infrastructure</TD>
<TD><a href='http://www.wings-workflows.org/'>http://www.wings-workflows.org/</a></TD>
<TD>https://github.com/varunratnakar/wings.git</TD>
<TD><a href='stats/wings/index.html'>stats</a></TD>
<TD>WINGS is a a semantic workflow system that assists scientists with the design of computational experiments. A unique feature of WINGS is that its workflow representations incorporate semantic constraints about datasets and workflow components, and are used to create and validate workflows and to generate metadata for new data products. WINGS submits workflows to execution frameworks such as Pegasus and OODT to run workflows at large scale in distributed resources</TD>
<TD>AGPL</TD>
<TR>
<TD>MIT-LL</TD>
<TD>Query By Example (Graph QuBE)</TD>
<TD>Analytics</TD>
<TD><a href='http://www.ll.mit.edu/mission/cybersec/HLT/HLT.html'>http://www.ll.mit.edu/mission/cybersec/HLT/HLT.html</a></TD>
<TD>Pending</TD>
<TD></TD>
<TD>Query-by-Example (Graph QuBE) on dynamic transaction graphs.</TD>
<TD></TD>
<TR>
<TD>MIT-LL</TD>
<TD>Julia</TD>
<TD>Analytics</TD>
<TD><a href='http://julialang.org/'>http://julialang.org/</a></TD>
<TD>https://github.com/JuliaLang/julia.git</TD>
<TD><a href='stats/julia/index.html'>stats</a></TD>
<TD>Julia is a high-level, high-performance dynamic programming language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.</TD>
<TD></TD>
<TR>
<TD>MIT-LL</TD>
<TD>tpoic</TD>
<TD>Analytics</TD>
<TD><a href='http://www.ll.mit.edu/mission/cybersec/HLT/HLT.html'>http://www.ll.mit.edu/mission/cybersec/HLT/HLT.html</a></TD>
<TD>Pending</TD>
<TD></TD>
<TD>Feature-conditioned topic co-clustering for integrating content and context features</TD>
<TD></TD>
<TR>
<TD>Next Century Corporation</TD>
<TD>Neon Visualization Environment</TD>
<TD>Visualization</TD>
<TD><a href='http://owfgoss.org/download.html'>http://owfgoss.org/download.html</a></TD>
<TD>https://github.com/ozoneplatform/owf.git</TD>
<TD><a href='stats/owf/index.html'>stats</a></TD>
<TD>Ozone Widget Framework is a customizable open-source web application that assembles the tools you need to accomplish any task and enables those tools to communicate with each other. It is a technology-agnostic composition framework for data and visualizations in a common browser-based display and interaction environment that lowers the barrier to entry for the development of big data visualizations and enable efficient exploration of large data sets.</TD>
<TD>ALv2</TD>
<TR>
<TD>Oculus Info Inc.</TD>
<TD>Aperture</TD>
<TD>Visualization</TD>
<TD><a href='http://aperturejs.com/'>http://aperturejs.com/</a></TD>
<TD>Pending</TD>
<TD></TD>
<TD>1. Zoomable Big Data Maps_x000D_
2. Statistical Summaries_x000D_
3. Financial Flow Analysis_x000D_
4. Zoomable Big Data Plots</TD>
<TD>MIT</TD>
<TR>
<TD>Raytheon BBN</TD>
<TD>Content and Context-based Graph Analysis: PINT, Patterns in Near-Real Time</TD>
<TD>Analytics</TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD></TD>
<TD>Patterns in Near-Real Time will take any corpus as input and quantify the strength of the query match to a SME-based process model. Represent process model as a Directed Acyclic Graph (DAG). Then search and score potential matches.</TD>
<TD></TD>
<TR>
<TD>Raytheon BBN</TD>
<TD>Content and Context-based Graph Analysis: NILS, Network Inference of LinkStrength</TD>
<TD>Analytics</TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD></TD>
<TD>Network inference of Link Strength will take any text corpus as input and quantify the strength of connections between any pair of entities. Link strengthÂ  probabilities are computed via shortest path.</TD>
<TD></TD>
<TR>
<TD>Scientific Systems Company, Inc.</TD>
<TD>BayesDB</TD>
<TD>Analytics</TD>
<TD>Pending</TD>
<TD>Pending</TD>
<TD></TD>
<TD>BayesDB is an open-source implementation of a predictive database table. It provides predictive extensions to SQL that enable users to query the implications of their data --- predict missing entries, identify predictive relationships between columns, and examine synthetic populations --- based on a Bayesian machine learning system in the backend. </TD>
<TD></TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Zephyr</TD>
<TD>Infrastructure</TD>
<TD><a href='http://github.com/Sotera/zephyr'>http://github.com/Sotera/zephyr</a></TD>
<TD>http://github.com/Sotera/zephyr</TD>
<TD><a href='stats/zephyr/index.html'>stats</a></TD>
<TD>Zephyr is a big data, platform agnostic ETL API, with Hadoop MapReduce, Storm, and other big data bindings.</TD>
<TD>ALv2</TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Page Rank</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/Sotera/page-rank'>https://github.com/Sotera/page-rank</a></TD>
<TD>https://github.com/Sotera/page-rank.git</TD>
<TD><a href='stats/page-rank/index.html'>stats</a></TD>
<TD>Sotera page-rank is a Giraph/Hadoop implementation of a distributed version of the Page Rank algorithm.</TD>
<TD>ALv2</TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Louvain Modularity</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/Sotera/distributed-louvain-modularity'>https://github.com/Sotera/distributed-louvain-modularity</a></TD>
<TD>https://github.com/Sotera/distributed-louvain-modularity.git</TD>
<TD><a href='stats/distributed-louvain-modularity/index.html'>stats</a></TD>
<TD>Giraph/Hadoop implementation of a distributed version of the Louvain community detection algorithm.</TD>
<TD></TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Spark MicroPath</TD>
<TD>Analytics</TD>
<TD>Pending</TD>
<TD>Pending</TD>
<TD></TD>
<TD>The Spark implementation of the micropath analytic</TD>
<TD></TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Arima</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/Sotera/rhipe-arima'>https://github.com/Sotera/rhipe-arima</a></TD>
<TD>https://github.com/Sotera/rhipe-arima</TD>
<TD><a href='stats/rhipe-arima/index.html'>stats</a></TD>
<TD>Hive and Rhipe implementation of an Arima analytic</TD>
<TD>ALv2</TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD> Leaf Compression</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/Sotera/leaf-compression'>https://github.com/Sotera/leaf-compression</a></TD>
<TD>https://github.com/Sotera/leaf-compression.git</TD>
<TD><a href='stats/leaf-compression/index.html'>stats</a></TD>
<TD>Recursive algorithm to remove nodes from a network where degree centrality is 1</TD>
<TD>ALv2</TD>
<TR>
<TD>Sotera Defense Solutions, Inc.</TD>
<TD>Correlation Approximation</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/Sotera/correlation-approximation'>https://github.com/Sotera/correlation-approximation</a></TD>
<TD>https://github.com/Sotera/correlation-approximation</TD>
<TD><a href='stats/correlation-approximation/index.html'>stats</a></TD>
<TD>Spark implementation of an algorithm to find highly correlated vectors using an approximation algorithm.</TD>
<TD>ALv2</TD>
<TR>
<TD>Stanford University - Boyd</TD>
<TD>QCML (Quadratic Cone Modeling Language)</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/cvxgrp/qcml'>https://github.com/cvxgrp/qcml</a></TD>
<TD>https://github.com/cvxgrp/qcml.git</TD>
<TD><a href='stats/qcml/index.html'>stats</a></TD>
<TD>Seamless transition from prototyping to code generation, Enable ease and expressiveness of convex optimization across scales with little change in code</TD>
<TD>unmarked</TD>
<TR>
<TD>Stanford University - Boyd</TD>
<TD>PDOS (Primal-dual operator splitting)</TD>
<TD>Analytics</TD>
<TD><a href='https://github.com/cvxgrp/pdos'>https://github.com/cvxgrp/pdos</a></TD>
<TD>https://github.com/cvxgrp/pdos.git</TD>
<TD><a href='stats/pdos/index.html'>stats</a></TD>
<TD>Concise algorithm for solving convex problems; solves problems passed from QCML</TD>
<TD>unmarked</TD>
<TR>
<TD>Stanford University - Hanrahan</TD>
<TD>datadr</TD>
<TD>Analytics</TD>
<TD><a href='http://hafen.github.io/datadr/'>http://hafen.github.io/datadr/</a></TD>
<TD>https://github.com/hafen/datadr.git</TD>
<TD><a href='stats/datadr/index.html'>stats</a></TD>
<TD>Complex big data are ubiquitous today. They challenge current numeric statistical and machine learning methods, visualization methods, statistical models, computational methods, and computational environments. D&R is being developed to meet these many challenges. In a D&R analysis, the data are divided into subsets in one more ways, forming multiple divisions. Numeric and visualization methods are applied to each of the subsets of a division, and the results of each method are recombined across subsets.</TD>
<TD>BSD Derivative</TD>
<TR>
<TD>Stanford University - Hanrahan</TD>
<TD>trelliscope</TD>
<TD>Visualization</TD>
<TD><a href='http://hafen.github.io/trelliscope/'>http://hafen.github.io/trelliscope/</a></TD>
<TD>https://github.com/hafen/trelliscope.git</TD>
<TD><a href='stats/trelliscope/index.html'>stats</a></TD>
<TD>Trelliscope provides a way to visualize large, complex data in great detail from within the R statistical programming environment. This package operates on data that has been split into subsets using the datadr package. Data can be local R data.frames or can be very massive RHIPE datasets. This is a research effort that is evolving, and this document communicates the current state.<br/><br/>Trelliscope is based on the concept of Divide and Recombine (D&R). Some philosophy and background on D&R can be found at datadr.org.</TD>
<TD>BSD Derivative</TD>
<TR>
<TD>Stanford University - Hanrahan</TD>
<TD>RHIPE</TD>
<TD>Infrastructure</TD>
<TD><a href='http://www.datadr.org/getpack.html'>http://www.datadr.org/getpack.html</a></TD>
<TD>https://github.com/saptarshiguha/RHIPE.git</TD>
<TD><a href='stats/RHIPE/index.html'>stats</a></TD>
<TD>RHIPE (hree-pay') is the R and Hadoop Integrated Programming Environment. It means "in a moment" in Greek. RHIPE is a merger of R and Hadoop. R is the widely used, highly acclaimed interactive language and environment for data analysis. Hadoop consists of the Hadoop Distributed File System (HDFS) and the MapReduce distributed compute engine. RHIPE allows an analyst to carry out D&R analysis of complex big data wholly from within R. RHIPE communicates with Hadoop to carry out the big, parallel computations.</TD>
<TD>unmarked</TD>
<TR>
<TD>Stanford University - Hanrahan</TD>
<TD>Riposte</TD>
<TD>Infrastructure</TD>
<TD><a href='https://github.com/jtalbot/riposte'>https://github.com/jtalbot/riposte</a></TD>
<TD>https://github.com/jtalbot/riposte.git</TD>
<TD><a href='stats/riposte/index.html'>stats</a></TD>
<TD>Riposte, a fast interpreter and JIT for R.</TD>
<TD>BSD-2</TD>
<TR>
<TD>Stanford University - Kunle</TD>
<TD>Delite</TD>
<TD>Infrastructure</TD>
<TD><a href='https://github.com/stanford-ppl/delite'>https://github.com/stanford-ppl/delite</a></TD>
<TD>https://github.com/stanford-ppl/Delite.git</TD>
<TD><a href='stats/Delite/index.html'>stats</a></TD>
<TD>Delite is a compiler framework and runtime for parallel embedded domain-specific languages (DSLs).</TD>
<TD>unmarked</TD>
<TR>
<TD>SYSTAP, LLC</TD>
<TD>bigdata</TD>
<TD>Infrastructure</TD>
<TD><a href='http://sourceforge.net/projects/bigdata/'>http://sourceforge.net/projects/bigdata/</a></TD>
<TD>https://bigdata.svn.sourceforge.net/svnroot/bigdata/</TD>
<TD><a href='stats/bigdata/index.html'>stats</a></TD>
<TD>Bigdata enables massively parallel graph processing on GPUs any many core CPUs. The approach is based on the decomposition of a graph algorithm as a vertex program. The initial implementation supports an API based on the GraphLab 2.1 Gather Apply Scatter (GAS) API. Execution is available on GPUs, Intel Xenon Phi (aka MIC), and multi-core GPUs. </TD>
<TD>GPLv2</TD>
<TR>
<TD>SYSTAP, LLC</TD>
<TD>mpgraph</TD>
<TD>Analytics</TD>
<TD><a href='https://sourceforge.net/projects/mpgraph/'>https://sourceforge.net/projects/mpgraph/</a></TD>
<TD>N/A</TD>
<TD></TD>
<TD>Mpgraph enables massively parallel graph processing on GPUs any many core CPUs. The approach is based on the decomposition of a graph algorithm as a vertex program. The initial implementation supports an API based on the GraphLab 2.1 Gather Apply Scatter (GAS) API. Execution is available on GPUs, Intel Xenon Phi (aka MIC), and multi-core GPUs. </TD>
<TD>ALv2</TD>
<TR>
<TD>UC Davis</TD>
<TD>gpugraph</TD>
<TD>Analytics</TD>
<TD>Pending</TD>
<TD>Planned Release 2013-10-28</TD>
<TD></TD>
<TD>Gpugraph is a CUDA library for graph primitives. This library is derived from the work by Duane Merrill on BFS traversal on GPUs and contains high performance CUDA implementations low level primitives for the best known algorithms for various graph primitives, including Breadth-First Search (contributed by Merrill), Connected Component, and Betweenness Centrality.</TD>
<TD></TD>
<TR>
<TD>The Charles Stark Draper Laboratory, Inc.</TD>
<TD>Activity Logger</TD>
<TD>Infrastructure</TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD></TD>
<TD>Activity Logger is an API to model analyst's workflow and operational context; Instrument tools to monitor analytic activities; Estimate user's arousal, cognitive load, engagement, task performance</TD>
<TD></TD>
<TR>
<TD>The New School</TD>
<TD></TD>
<TD></TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD></TD>
<TD></TD>
<TD></TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>BDAS</TD>
<TD>Infrastructure</TD>
<TD><a href='https://amplab.cs.berkeley.edu/software/'>https://amplab.cs.berkeley.edu/software/</a></TD>
<TD>N/A</TD>
<TD></TD>
<TD>BDAS, the Berkeley Data Analytics Stack, is an open source software stack that integrates software components being built by the AMPLab to make sense of Big Data.</TD>
<TD></TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>Spark</TD>
<TD>Infrastructure</TD>
<TD><a href='http://spark.incubator.apache.org/'>http://spark.incubator.apache.org/</a></TD>
<TD>https://github.com/mesos/spark.git</TD>
<TD><a href='stats/spark/index.html'>stats</a></TD>
<TD>Apache Spark is an open source cluster computing system that aims to make data analytics fast â both fast to run and fast to write. To run programs faster, Spark offers a general execution model that can optimize arbitrary operator graphs, and supports in-memory computing, which lets it query data faster than disk-based engines like Hadoop. To make programming faster, Spark provides clean, concise APIs in Python, Scala and Java. You can also use Spark interactively from the Scala and Python shells to rapidly query big datasets.</TD>
<TD>ALv2</TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>Shark</TD>
<TD>Infrastructure</TD>
<TD><a href='https://github.com/amplab/shark'>https://github.com/amplab/shark</a></TD>
<TD>https://github.com/amplab/shark.git</TD>
<TD><a href='stats/shark/index.html'>stats</a></TD>
<TD>Shark is a large-scale data warehouse system for Spark designed to be compatible with Apache Hive. It can execute Hive QL queries up to 100 times faster than Hive without any modification to the existing data or queries. Shark supports Hive's query language, metastore, serialization formats, and user-defined functions, providing seamless integration with existing Hive deployments and a familiar, more powerful option for new ones.</TD>
<TD>ALv2</TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>BlinkDB</TD>
<TD>Infrastructure</TD>
<TD><a href='http://blinkdb.org/'>http://blinkdb.org/</a></TD>
<TD>Pending</TD>
<TD></TD>
<TD>BlinkDB is a massively parallel, approximate query engine for running interactive SQL queries on large volumes of data. It allows users to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas: (1) An adaptive optimization framework that builds and maintains a set of multi-dimensional samples from original data over time, and (2) A dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy and/or response time requirements. We have evaluated BlinkDB on the well-known TPC-H benchmarks, a real-world analytic workload derived from Conviva Inc. and are in the process of deploying it at Facebook Inc. </TD>
<TD></TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>Mesos</TD>
<TD>Infrastructure</TD>
<TD><a href='http://mesos.apache.org/'>http://mesos.apache.org/</a></TD>
<TD>https://git-wip-us.apache.org/repos/asf/mesos.git</TD>
<TD><a href='stats/mesos/index.html'>stats</a></TD>
<TD>Apache Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications, or frameworks. It can run Hadoop, MPI, Hypertable, Spark, and other applications on a dynamically shared pool of nodes.</TD>
<TD>ALv2</TD>
<TR>
<TD>University of California, Berkeley</TD>
<TD>Tachyon</TD>
<TD>Infrastructure</TD>
<TD><a href='https://github.com/amplab/tachyon'>https://github.com/amplab/tachyon</a></TD>
<TD>https://github.com/amplab/tachyon.git</TD>
<TD><a href='stats/tachyon/index.html'>stats</a></TD>
<TD>Tachyon is a fault tolerant distributed file system enabling reliable file sharing at memory-speed across cluster frameworks, such as Spark and MapReduce. It achieves high performance by leveraging lineage information and using memory aggressively. Tachyon caches working set files in memory, and enables different jobs/queries and frameworks to access cached files at memory speed. Thus, Tachyon avoids going to disk to load datasets that are frequently read.</TD>
<TD>BSD derivative</TD>
<TR>
<TD>University of Southern California</TD>
<TD>goffish</TD>
<TD>Infrastructure</TD>
<TD><a href='https://github.com/usc-cloud/goffish'>https://github.com/usc-cloud/goffish</a></TD>
<TD>https://github.com/usc-cloud/goffish.git</TD>
<TD><a href='stats/goffish/index.html'>stats</a></TD>
<TD>The GoFFish project offers a distributed framework for storing timeseries graphs and composing graph analytics. It takes a clean-slate approach that leverages best practices and patterns from scalable data analytics such as Hadoop, HDFS, Hive, and Giraph, but with an emphasis on performing native analytics on graph (rather than tuple) data structures. This offers an more intuitive storage, access and programming model for graph datasets while also ensuring performance optimized for efficient analysis over large graphs (millions-billions of vertices) and many instances of them (thousands-millions of graph instances).</TD>
<TD>ALv2</TD>

</tbody> 
</table>
</html>
